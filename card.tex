\documentclass[12pt, a4paper,twoside]{report}

%% Every LaTeX document begins with a preamble, which loads packages and
%% defines various settings to make the document look right. Mostly,
%% you can ignore everything in this template before \begin{document} on line 74

\usepackage{mathtools,amsthm,amsfonts} % Enable useful mathematical symbols/environments
\usepackage{amsmath}
\usepackage{graphicx} % Enable graphics
\usepackage{fancyhdr,titlesec,microtype} % enable various formatting commands
\usepackage[margin=2.5cm]{geometry} % Set margin size
\usepackage{palatino} % Set the font
\usepackage[latin1]{inputenc} % Allow you to input accents, umlauts and other characters
\usepackage[T1]{fontenc} % Lets LaTeX print a wider array of characters
\usepackage{tikz,tikz-3dplot,tkz-euclide} % Enable tikz drawings
\usepackage{url}
% \usepackage{natbib}

\usepackage[backend=biber, style=authoryear, sorting=nyt]{biblatex}
\addbibresource{cvd.bib}

\usepackage{xcolor} % Enable coloured elements
\usepackage{listings}
%\usepackage{multirow}
\definecolor{mypurple}{HTML}{622567} %%% Purple
\definecolor{myred}{HTML}{D55C19} %%%EssexOrange
\definecolor{myblue}{HTML}{007A87} %%Seagrass

% For technical reasons, hyperref should be loaded after all other packages
\usepackage[colorlinks,linkcolor=myblue,citecolor=mypurple]{hyperref}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\renewcommand{\baselinestretch}{1.5} % 1.5 line spacing

\numberwithin{equation}{chapter}

% Fancy headings
\pagestyle{fancy}
\setlength{\headheight}{15pt}
\fancyheadoffset[LE,RO]{0pt}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\fancyhf{}
\fancyhead[LE]{\makebox[0pt][l]{\thepage}\hfill\leftmark}
\fancyhead[RO]{\rightmark\hfill\makebox[0pt][r]{\thepage}}
\fancypagestyle{plain}{%
    \fancyhead{} % get rid of headers
    \renewcommand{\headrulewidth}{0pt} % and the line
}

% Fancy chapter numbers
\titleformat{\chapter}[display]
    {\normalfont\bfseries\color{myred}}
    {\filleft\hspace*{-60pt}%
        \rotatebox[origin=c]{90}{%
            \normalfont\color{black}\Large%
            \textls[180]{\textsc{\chaptertitlename}}%
        }
        \hspace{10pt}%
        {\setlength\fboxsep{0pt}%
            \colorbox{myred}{\parbox[c][3cm][c]{2.5cm}{%
                \centering\color{white}\fontsize{80}{90}\selectfont\thechapter}%
            }
        }
    }
    {10pt}
    {\titlerule[2.5pt]\vskip3pt\titlerule\vskip4pt\LARGE\sffamily}

\begin{document} % Start your document

%%%%%%%%%%%% BEGIN TITLE PAGE %%%%%%%%%%%%

\thispagestyle{empty} % For the title page, no header / footer

\noindent
    \begin{minipage}{0.1\textwidth}
    \includegraphics[height=4.5em]{essex.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.5\textwidth}
    \begin{center}
        \renewcommand\familydefault{\sfdefault}
        \fontfamily{phv}\selectfont
        {\large School of Mathematics, Statistics and Actuarial Science}
    \end{center}
    \end{minipage}

\begin{center}
    \noindent\textcolor{myred}{\rule{\linewidth}{4.8pt}}
    
    \vspace{2em}
    \noindent {\LARGE \sc MA981 Dissertation}
    
    \vspace{3em}
    \noindent {\Huge{\color{myblue} CARDIOVASCULAR DISEASE RISK PREDICTION}}
    
    \vspace{3em}
    \noindent {\Large \bf AMALA JELFA JOSEPH PAUL NEVIL}
    
    \vspace{3em}
    \noindent {\Large \bf Registration Number: 2311832}
    
    \vfill
    \noindent {\Large {Supervisor:} {\color{mypurple} \bf DR. RISHIDEEP ROY}}
    
    \vspace{0.5em}
    \noindent\textcolor{myred}{\rule{\linewidth}{4.8pt}}
    
    \vspace{2em}
    {\Large \today }
    
    {\Large Colchester}
\end{center}

\clearpage

%%%%%%%%%%%% END TITLE PAGE %%%%%%%%%%%%

\tableofcontents

% If you have lots of figures with captions / numbers, uncomment the following line
% \listoffigures

% If you have lots of tables and want a list of them, uncomment the following line
% \listoftables

\begin{center}
    \newpage
    \vspace{0.9cm}
    \Large \bf{Abstract}
\end{center}

This study explores the use of many machine learning models in a comparative analysis to predict cardiovascular risk. Models such as AdaBoost, XGBoost, Random Forest, Decision Tree, and Logistic Regression are compared. The models' capacity to correctly identify people at high risk for cardiovascular problems is highlighted by the research, which assesses the models using important metrics including recall, precision, and ROC-AUC. According to our research, ensemble approaches perform better in this situation, especially AdaBoost and XGBoost. AdaBoost has the best recall and ROC-AUC scores, making it the most dependable model for detecting high-risk patients. Strong performance was also shown using logistic regression, indicating that it is resilient for classification tasks in the medical field. In contrast, the underperformance of Random Forest and Decision Tree models can be attributed to their difficulties in identifying intricate patterns linked to cardiovascular risk factors.

\chapter{Introduction}\label{ch:1}


Cardiovascular refers to the heart and the blood vessels. This system is responsible for transporting essential substances such as nutrients, oxygen and hormones to the body cells for use. Like all parts of the human body, the cardiovascular system can suffer from disorders which affect the heart and blood vessels, these disorders can encompass a myriad of conditions such as Coronary heart disease, Stroke, Cerebrovascular disease, Peripheral artery disease, Aortic disease, Arrhythmia, Deep vein thrombosis, Pulmonary embolism, Rheumatic heart disease, Congenital heart disease and heart failure\parencite{cambridge}.

Data from the Centres for Disease Control and Prevention \parencite{cdc} show relatively high percentages of death due to cardiovascular diseases, with varying percentages for different races. Blacks had a high death rate of 22.6\%, followed by Asians at 18.6\%, Native Hawaiians at 18.3\%, Whites at 18\%, American Indians at 15.5\% and other races making up 17.4\% of all deaths caused by heart disease. The number of deaths due to CVD has been on the rise since 1990 with an estimated 17.9 million lives taken each year \parencite{WHO_2019}.

\section{Risk factors}
Several risk factors play a significant role in increasing the susceptibility of a person to CVDs. Some factors can be outlined as the person's age, gender and family history. Certain lifestyle choices can also influence susceptibility to CVDs such as a high-cholesterol diet \parencite{Actrn2019}, high intake of alcohol \parencite{Tegegne2022}, little to no physical activity \parencite{Mangione2022}, chronic stress \parencite{An2016}, obesity \parencite{lopez2022}, and substance abuse such as cigarettes and tobacco \parencite{Salahuddin2012}. Conditions such as hypertension \parencite{Wang2020}, and diabetes \parencite{Kilkenny2017} also increase the susceptibility to cardiovascular diseases.

\subsection{Age}
According to several sources and research, it is evident that the age of a person is a potent risk factor for CVDs. \parencite{curtis2018arrhythmias} in their research show that people above the age of 80 suffer from impairment of functionalities in their cardiovascular system. They experience a decrease in the elasticity of their arterial walls leading to arterial stiffness. The myocardium, which is the muscular tissue of the heart also undergoes significant structural changes \parencite{Ribeiro2023} which prevents efficient contraction and relaxing of the heart, ultimately leading to heart failure \parencite{Tromp2021}.

Older patients are more likely to suffer from multiple comorbidities namely hypertension and diabetes which long-term cumulative exposure contributes to a higher incidence of CVD. There is a strong relationship between these two factors and a strong need to implement early intervention to mitigate the risks for the elderly.

\subsection{Smoking}

Cigarette smoking is also another risk factor which contributes to the development of cardiovascular diseases. Cigarettes contain many chemicals, up to 7000 when burned \parencite{ALA} of which 70 are cancer-causing chemicals \parencite{ALA, CANCERGOV}. These chemicals that are passed into the body during smoking can cause an unhealthy buildup of fatty deposits in the arteries of the heart leading inevitably to a condition known as Atherosclerosis \parencite{Rafieian-Kopaei2014, Salehi2021} which can also cause stroke. The Atherosclerosis condition also leads to hardening and narrowing of arteries \parencite{Australia}

A certain substance found in Cigarettes, Nicotine, is shown to have a direct negative effect on blood pressure. \parencite{Price_Martinez_2020} describes the impact of nicotine on the cardiovascular system, stating that "nicotine primarily acts on the cardiovascular system through stimulation of the sympathetic nervous system leading to the release of norepinephrine and increases in heart rate, blood pressure, myocardial contractility and systemic vasoconstriction." Nicotine over time also reduces the O2 binding capacity of red blood cells (RBC) caused by abnormal changes in its morphology \parencite{Aldosari_Ahmadal_2020}. When the oxygen-carrying capacity of RBC is reduced, organs in the body such as the heart cannot receive the necessary amount of blood and oxygen to function properly, leading to ischemic CVD complications.

\subsection{Diabetes}
Diabetes Mellitus is a condition caused when the pancreas is unable to produce enough insulin (Type 1) or when the body fails to effectively use insulin (Type 2). Insulin helps in the regulation of blood sugar and a lack of it leads to dangerous and unhealthy levels of blood sugar \parencite{Society_2023}. While Type 1 diabetes is linked to genetic predisposition \parencite{Simmons_2015}, the risk of developing Type 2 diabetes is exacerbated by unhealthy lifestyle changes involving diet, obesity and physical inactivity. Elevated blood glucose levels in the body are also one of the causes of atherosclerosis in the body potentially leading to heart attacks and strokes.

Research shows that the presence of diabetes causes a patient to have an abnormal lipid profile known as diabetic dyslipidemia \parencite{Bhowmik_Siddiquee2018} which entails high levels of triglycerides, low levels of HDL cholesterol and high levels of low-density lipoprotein (LDL) cholesterol. This profile contributes to an increase in cardiovascular risk by clogging arteries and reducing blood flow to the heart.

\subsection{Exercise}
Exercise is crucial in keeping the human body fit and burning off exercise calories to maintain a healthy weight. The latest global report on physical activity \parencite{who2022} reports a lack of the recommended levels of activity in 80\% of adolescents and 27\% of adults.

The lack of physical activity or exercise is a major contributor to the development of obesity in an individual \parencite{Kazmi_Nagi2022}. Regular exercise helps an individual to maintain healthy levels of blood pressure by improving blood flow and hence, also improving the elasticity of blood vessels which inactive individuals lack \parencite{Hegde_Solomon_2015}. Lack of healthy blood flow and stiffer arteries in active people can cause hypertension \parencite{Gamage2021}. 

Research by \parencite{Scher2019} shows that physical inactivity can lead to metabolic dysfunction by causing a negative impact on lipid profiles, also leading to the accumulation of harmful lipids in the blood \parencite{DaSilva2022}. This would then cause an increased risk of having coronary artery disease. This highlights a significant relationship between physical inactivity and CVD.

\section{Aim of the Study}
This study aims to train a machine learning model using a CVD dataset to predict with good accuracy the risk of a person having cardiovascular disease. This study will also perform a comparative analysis between several machine learning algorithms to determine which algorithm can demonstrate the best performance.



\chapter{Literature Review}\label{ch:2}

\section{Overview}
CVD is established as a leading cause of death globally and this necessitates the need for early intervention techniques to reduce its impact on high-risk individuals. There has been recent advancements in machine learning in the healthcare industry and \parencite{CHAKRABORTY2024100164} describes it as "playing a crucial role in the paradigm shift in medicine". The use of machine learning (ML) in the healthcare industry has assisted greatly in the early identification of diseases and better diagnosis of diseases \parencite{olaoyelucas2024}. The field of CVD is not left out of these advancements in ML, for example, it has seen advancements such as Electrocardiography (ECG) and cardiovascular image analysis \parencite{9791776}.

This section will contain a review of the machine learning models to be used in this study as well as a review of related works by other authors in this field.

\section{Algorithms to be used in the Study}

\subsection{Logistic Regression}
The logistic regression algorithm is a statistical method commonly used to solve binary classification problems using a logistic function i.e binary classification involves the classification of a set of features into one of two distinct categories \parencite{Wikipedia_2024}. 

\subsection{Decision Tree}
The Decision Tree algorithm \parencite{Quinlan1986Induction} is a supervised learning algorithm which works by splitting a dataset into a subset by using the most important features at each step. The splits are made recursively till the final prediction is arrived at.

It is expressed as a recursive split of the instance space \parencite{cadena-2016}. The decision tree consists of nodes that join to form a directed tree called a "rooted tree," which has a node called the "root" that has no edges entering it. Every other node has precisely one incoming edge \parencite{dao-tran-2015}. A node with outward edges is called an internal or test node. All other nodes are leaves, often called terminal or decision nodes. A decision tree's internal nodes split the instance space into two or more sub-spaces according to a certain discrete function of the input attribute values \parencite{pratola-2016}. Each leaf has been allocated a class based on the optimal target value. Alternatively, a probability vector indicating the likelihood that the target characteristic would have a specific value could be present in the leaf \parencite{van-engelen-2019}. Instances are categorized by moving down the tree from the base to a leaf, using the outcomes of the tests along the way. Geometrically speaking, decision trees have numerical properties similar to a collection of hyperplanes that are all perpendicular to one of the axes \parencite{basu-2023}.


\subsection{Random Forest}
The random forest algorithm adopts an ensemble approach by independently training many strong learners usually decision trees, as outlined by \parencite{Toprak_2021} after which a prediction is derived through the average weighting of all predictions \parencite{AnalytixLabs_2023} by each unique learner in the ensemble.

Binary recursive partitioning trees serve as an inspiration for random forest trees. These trees solely manage the predictor space division, which is accomplished by a sequence of binary partitions on various variables \parencite{zappone-2019}. The root node in the random forest algorithm symbolizes the entire predictor space. Because they mark the end of a route and produce the final divide in the predictor space, the nodes that survive are known as "terminal nodes" \parencite{zhu-2023}. Depending on the value of one of the predictor variables, each non-terminal node is divided into two descendant nodes on the left and right using this technique \parencite{gupta-2015}. If the continuous variable is continuous, then a point where the predictor is smaller than the split-point travels to the left and the rest points move to the right \parencite{cadena-2016}. These split-points establish the separation. The tree considers all possible splits on each predictor variable and selects the optimal one based on a set of criteria to choose which split to employ to split a node into two descendants \parencite{jena-2020}\

The process of creating a random forest involves the generation of multiple decision trees. We train each of the tree on it's own different subset of the training data in a process called bootstrapping. Bootstrapping means to randomly sample the data with replacement from the training data.

During the splitting of the data at each node of a tree, not all k features are considered at once and only a random subset of the features are chosen. The best split is found by evaluating the subset, which helps to introduce randomness, reduces the correlation between the trees (also helping to lower the variance of the model) and preventing overfitting.

Each tree T is constructed using the dataset D and the random feature subset F and the splitting process continues until it satisfies the condition of the stopping criterion which can vary from options such as the pure nodes or maximum depth allowed.


The final prediction of the random forest model is made through the aggregation of all predictions made by each tree. 

In classification, each tree in the forest votes for a prediction class. The class with the most votes will become the random forest's final prediction.

In regression, all predictions from each tree are averaged to arrive at the final prediction.

\subsection{XGBoost}
The extreme Gradient Boosting Algorithm \parencite{chentianqi} is an advanced and improved version of the typical Gradient Boosting Algorithm and it works by constructing multiple decision trees in a sequential manner in which the succeeding tree focuses on correcting the mistakes of the preceding for up to a certain number of iterations. 

This system is accessible for all popular data analysis languages and performs remarkably well for multi class classification problems \parencite{warnat-herresthal-2021}. A machine learning method called XGBoost is a component of the ensemble learning subset gradient boosting architecture \parencite{mehta-2019}. Let us quickly review ensemble learning first. As the name suggests, a "ensemble" or collection of "weaker" models is used in ensemble learning to create a more strong model \parencite{vilarino-2017}. One boosting method that creates a strong model by repeatedly learning from each of the weak learners is gradient boosting. Using decision trees as foundation learners, it applies regularization techniques to enhance model generalization \parencite{ji-2022}. XGBoost is widely recognised for its computational efficiency, which enables efficient processing, insightful feature importance analysis, and seamless handling of missing information. This method is the best option for many applications, including regression, classification, and ranking. Weak learners are progressively added to the ensemble in order for the algorithm to work, with each new learner focussing on correcting the errors made by the prior ones \parencite{zhang-2019}.


\subsection{AdaBoost}
The adaptive boosting algorithm \parencite{freund1997decision} is an ensemble method, unlike Random Forest that combines many weak learners to create a single strong learner. This algorithm works by adjusting the weights of incorrectly classified data points by each weak learner thereby increasing their importance to the next learner.

Weak classifiers can frequently be specified for a given task. For example, one can look for a "weak classifier" that works 60\% of the time. Alternatively, and more broadly, to define a big dictionary of poor classifiers from which AdaBoost can choose a small number to create a strong classifier that is accurate 99\% of the time \parencite{sepulveda-oviedo-2023}
AdaBoost is adaptive in that it adjusts weaker learners in the future to take into account cases when prior classifiers misclassified the data \parencite{van-engelen-2019}. Compared to other learning algorithms, it may be less prone to the overfitting issue in specific situations \parencite{krawczyk-2016}. It can be demonstrated that the overall model converges to a strong learner even if the performance of each individual learner is only marginally better than chance \parencite{hullermeier-2021}. It has been demonstrated that AdaBoost can successfully integrate strong base learners, like deep decision trees, as well, leading to an even more accurate model, even though it is usually employed to combine weak base learners \parencite{mienye-2022}.
    
Here, weak classifiers are combined with linear weights because there is an efficient algorithm that allows this and additionally, this technique seems to provide a classifier that is well-generalized even with little amounts of data \parencite{schmidt-2019}. 

AdaBoost is the most widely used boosting algorithm, it gets its name from its "adaptive" nature \parencite{mehta-2019}. Compared to SVMs, AdaBoost is significantly easier to use and implement, and it frequently produces very good results \parencite{sagi-2018}.




\section{Review of Related works}
\subsection{ECG}

In \parencite{Jin_Sun_Cheng_2009} the authors were able to develop an adaptive artificial neural network (ANN) capable of classifying an individual based on his CVD risk by the use of information from clinical ECG databases and a cell phone that monitors CVD in real time to generate a cardiac health summary. This approach was able to achieve an impressive accuracy of $90\%$. In 2023, \parencite{9735300} sought to build a model to classify ECG images into four cardiac abnormalities. They introduced transfer learning using SqueezeNet and AlexNet while also building a custom CNN architecture that outperformed the state-of-the-art models with a $98.23\%$ accuracy, $98.22\%$ recall, $98.31\%$ precision and $98.21\%$ F1 score.

By performing a comparative analysis of 6 machine learning algorithms, \parencite{NazrHafiMohd20209i} achieved $90\%$ specificity, $90\%$ sensitivity and $90\%$ accuracy on ANN by using ECG data from a Malaysian multiethnic population.


\subsection{Stacking}
\parencite{Subramani2023} in 2023, proposed a stacking approach to the predicting of CVD using a dataset combined from various sources. The authors used the Logistic Regression (LR) model as the meta learner in the stacking model while Random Forest (RF), Linear Regression (LR), Decision Tree (DT), Multilayer Perceptron (MLP) and CatBoost. After feature selection with SHAP and a 5-fold CV for training, it was determined that adopting a stacking approach for the prediction of CVD demonstrated better performance than the use of individual machine-learning models. \parencite{sun2024} in their recent research also adopted the use of a stacking model with LR, DT, Gradient Boosting (GBT), Adaptive Boosting (ADB), eXtreme Gradient Boosting (XGBoost) and Deep Neural Network(DNN) and was able to achieve high metric scores with a precision of $86.80\%$, recall of $84.78\%$ and an f1 score of $85.76\%$.

\parencite{jpm13020373} devised a novel approach known as a multi-modal stacking ensemble for cardiovascular disease prediction which makes use of two modalities: scalogram images and ECG grayscale images derived from an ECG signal. These two modalities are used to make predictions in the stacking ensemble which uses a pretrained ResNet-50 model as the base learner and other models such as LR, Support Vector Machine (SVM), RF and XGBoost as meta learner and combined. The study was able to achieve high metric stores such as an F1 of $0.936$, AUC of $0.995$, accuracy of $93.9\%$, sensitivity of $0.940$, and precision of $0.937$.


\subsection{CRNN}
In 2023, \parencite{shuvo} pioneered a fresh perspective on the use of Phonocardiogram (PCG) signals for the prediction of CVD. They introduced a Convolutional Recurrent Neural Network (CRNN) derived from the combination of Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). This architecture consisted of three parallel CNN pathways: Frequency Feature Extractor (FFE), Pattern Extractor (PE) and Adaptive Feature Enhancer (AFE) for representation learning and two layers of Bi-Lateral LSTMS (Bi-LSTMS) for sequential residual learning and was able to state of the art performance with an accuracy of $99.60\%$, precision of $99.56\%$, recall of $99.52\%$ and an F1 of $99.68\%$. 

\parencite{10409163} built on the work of \parencite{shuvo} and named it a custom scalogram CRNN (CS-CRNN) which involved the addition of a custom scalogram layer to break down PCG signals into different scales of a wavelet and to analyze the strength of each signal at each level. The next layers of the model involved 3 Convolutional layers, 2 LSTM layers and 5 Dense layers. This led to a performance of $99.6\%$ accuracy for binary classification, which is believed to be comparable to other known methods.

\subsection{Other methods}
In \cite{dritsas2022cardiovascular}, the authors based their aim on the risk evaluation for CVD occurrence on male and female participants who are older than 50 years with the aid of machine learning. In this research, the authors proposed a predictive approach using classical machine learning techniques such as Naive Bayes model, support vector machine, logistic regression and random forest. The dataset utilized by the researchers was the CVD dataset which is an open source dataset obtained from kaggle online repository which is readily available for machine learning training and testing. The dataset is a balanced dataset which consists of equally healthy patients and those diagnosed with CVD. It contains a total of 11 features which are 4 demographic, 4 examinations and 3 social history which includes the age, height, gender and weight of the patient, which would be used to calculate the body mass index (BMI) of the patients. It also comprises some physical and chemical information about the state of the individuals such as glucose level, physical activity, drinking and smoking habits, systolic and diastolic blood pressure, all of which were attributed categorical results which would aid the determination of the cardiovascular disease. The data pre-processing was done using the stata v.14 toolkit which is a general purpose statistical software package used for data manipulation, visualization, statistics and automated reporting. To establish a normalized and efficient table of data features, a harmonized data for each data attribute was created, transforming all numerical values to nominal values, after these steps were taken the total observations were 70,000. Feature selection was done using random forest on the selected features which ranks all attributes with respect to the relevance to the specific class. After this the dataset was split into two parts, training and testing set in the ratio 70:30. The evaluation metrics realized after the training and testing showed Logistic regression performing more than the other models with an accuracy of 72.06\%, followed by the Random Forest with 70.86\%, the support vector machine with 70.61\% and lastly the Naive Bayes model with 59.59\%.

In 2017, the authors of \cite{weng2017can} proposed a predictive approach to check for the improvement of cardiovascular risk prediction using machine learning using routine clinical data. The authors sourced the data from the Clinical Practice Research Datalink (CPRD), anonymised electronic medical records from about 700 family practices in the UK that include information on demographics, medical histories, prescription medications, acute medical outcomes, hospital admissions, specialist referrals, and biological data. The database is connected to secondary care hospital records and is typical of the general population in the United Kingdom. In the established ACC/AHA 10-year risk prediction model, the eight core baseline variables (gender, age, smoking status, systolic blood pressure, blood pressure treatment, total cholesterol, HDL cholesterol, and diabetes) were fully recorded for the cohort of patients, who were registered with a family practice between the ages of 30 and 84 at baseline. The principal result was the initial diagnosis of a cardiovascular event, whether fatal or non-fatal, entered into the patient's primary or secondary care electronic health record. The UK National Health Service (NHS) electronically records and labels CVD in primary care, examine the codes with the sole purpose of comparing the machine learning algorithms to determine the best. the dataset was split into training, validation and finally testing set. The train set was derived from a random sample of 75\% of the extract data and the validation data was sampled from the 25\% CPRD cohort. The authors utilized three machine learning algorithms and a neural network which were logistic regression, random forest, gradient boosting and neural networks. The results, after being evaluated using the area under the curve (AUC), showed the neural network with the highest AUC of 76.4, followed by the gradient boosting machine with 76.1, logistic regression with 76.0, and random forest classifier with 74.5. These models were compared using the ACC/AHA which is a baseline model and they all performed better than the baseline model which had an AUC of 72.8 itself.

In \cite{pal2022risk}, the authors proposed a comparative analysis between two machine learning models, the multi layer perceptron (MLP) and the k nearest neighbor (KNN) for the detection of cardiovascular diseases. The researchers sourced the data from the University of California Irvine (UCI) repository which contains 303 samples and 76 attributes. The dataset was imbalanced due to the fact that only 138 patients were normal while 165 patients had cardiovascular disease. Some important features the authors took note of were age, sex, chest pain, resting blood pressure, cholesterol, blood sugar levels while fasting, resting electrocardiograph results, max heart rate, exercise induced angina, depression, thallium stress results, and number of major vessels. The authors took some preliminary steps in cleaning and preprocessing the dataset such as smoothing, standardization, and aggregation as the dataset had some noise and missing information. After the training session the accuracy of the k nearest neighbor (KNN) and the multilayer perceptron (MLP)  was seen to be $73.77\%$ and $82.47\%$ respectively. The area under the curve was also accounted for with the multilayer perceptron (MLP) being higher with $86.41$ while the k nearest neighbor (KNN) emerging with only $86.21$.

The researchers of \cite{alaa2019cardiovascular} proposed a predictive approach concentrating on enhancing the risk prediction of cardiovascular disease (CVD) by utilizing machine learning (ML) approaches. In particular, it entails determining if machine learning based strategies can outperform conventional approaches in cardiovascular disease risk prediction by utilizing an automated machine learning framework also known as Auto Prognosis. It also investigates the possibility of incorporating non-traditional elements to improve the accuracy of these forecasts. Based on 473 available factors, the authors created an ML-based model for predicting CVD risk using data from 423,604 participants in UK Biobank who did not have CVD at baseline. Auto Prognosis, an algorithmic tool that automatically chooses and tunes ensembles of ML modeling pipelines (comprising data imputation, feature processing, classification and calibration algorithms), was used by the researchers to derive their ML-based model. The Cox proportional hazards (PH) model based on known risk factors (e.g., age, gender, smoking status, systolic blood pressure, history of diabetes, receipt of treatments for hypertension, and body mass index) and the well-known risk prediction algorithm based on conventional CVD risk factors (Framingham score) were compared with the authors' model. The authors avoided over-fitting by comparing and evaluating the results of all the models under 10 fold stratified cross validation using the Receiver Operating Characteristic Area under the curve (ROC-AUC). The main models, support vector machine (SVM), random forest classifier, neural networks, Adaboosting and gradient boosting had an AUC-ROC score of 70.9, 73.0, 75.5, 75.9 and 76.9 respectively.

In \cite{pasha2020cardiovascular}, the researchers proposed an approach to predict cardiovascular disease (CVD) using machine learning and deep learning algorithms. The researchers utilized a publicly available dataset sourced from kaggle which is an online database for machine learning, the dataset contained several attributes which could be seen as factors that could be related to heart disease such as age, gender, blood pressure, and cholesterol. The dataset was split into a ratio 80:20 for the training and testing process. The machine learning algorithms used in this paper were the artificial neural network (ANN) which is also a multilayer perceptron, support vector machine (SVM), k nearest neighbor (KNN), and decision trees classifier. After the training and testing phase, the accuracies were seen to be 81.97\%, 67.2\%, 81.97\% and 85.24\% for the support vector machine (SVM), k nearest neighbor (KNN), decision trees classifier and the artificial neural network respectively.

In \cite{quesada2019machine}, the authors proposed a predictive approach to determine, predict and prevent cardiovascular risks using 15 machine learning algorithms. Analytical observational analysis of the ESCARVAL RISK clinical practice cohort was designed by the authors, who tracked primary care patients from January 2008 until December 2012. Patients 40 years of age or older with a diagnosis of diabetes, dyslipidemia, or hypertension made up the study cohort. During the inclusion phase, the study variables were gathered from the electronic health records of the patients. Age, sex, chest pain, resting blood pressure, cholesterol, blood sugar levels while fasting, resting electrocardiograph findings, maximum heart rate, exercise-induced angina, depression, thallium stress results, and the number of main vessels were some of the significant characteristics that the researchers took note of to assess risk for the score assessment. With an AUC of 0.7086, Quadratic Discriminant Analysis is the most predictive approach followed by Naive Bayes and Neural Networks with AUCs of 0.7084 and 0.7042, respectively. Three of the 15 ML techniques that were examined have AUC values that are less than 0.6.


In the paper \cite{goldstein2017moving}, the researchers proposed a method to portray the use of machine learning algorithms for the development of risk predictive systems. The researchers utilized a dataset obtained from the EHR system of the Duke University medical center and abstracted information on the demographics of the patients such as their age, gender and race. The overall population used for the analysis was about 1944 patient counts. Thirteen laboratory tests (calcium, carbon dioxide, creatinine, creatinine kinase-mb, hemoglobin, glucose, mean corpuscular volume, mean corpuscular hemoglobin concentration, platelet count, potassium, red blood cell distribution width, sodium, and white blood cell count) were found to be present in at least 80\% of the sample and after the calculation of some statistical values, the authors found the predictor variables to be 43. After the training and testing phase, the machine learning algorithms were evaluated using mainly the square error loss as the main metric. The proposed models were logistic regression, forward selection, LASSO, RIDGE, PCR, general additive models, CART, random forest classifier, boosting, k nearest neighbor and neural networks which gave a square error of 49.0, 46.0, 46.0, 47.0, 49.0, 50.0, 53.0, 48.0, 47.0, 50.0, 65.0 respectively. The models with the lowest errors were the forward selection and the LASSO with both a error of 46.0.


\chapter{Data description}
In this chapter, we will discuss the many research methodologies that constitute the foundation of this study. This chapter will begin by describing the dataset chosen, its qualities, and its significance to the study. 

This step is followed by the dataset analysis which will detail how the dataset is processed and interpreted to derive meaningful conclusions which will be useful in later aspects of the study.
The next step after this is to discuss the in-depth workings of the 5 models used in this study namely: Logistic Regression, Random Forest, Decision Tree, AdaBoost, XGBoost. We will visit the underlying mathematical aspects of the models and how they work to make predictions.

\section{Dataset}

The dataset being used in this study is originally from the paper \parencite{subramani_varshney2023} which was then cleaned, pre-processed by \parencite{harshwardhanfartale_2023} before he uploaded to an online dataset database popular referred to as kaggle. It is not detailed which specific pre-processing steps were used to clean the dataset but a glance at the dataset suggests that there is still a need for more pre-processing to suit the specific use case on this study.

The dataset consists of 19 columns and 308854 rows of data. The 19 columns present in the dataset contain information useful towards the prediction of the risk of cardiovascular disease and they are as follows: General Health, Checkup, Exercise, Heart Disease, Skin Cancer, Other Cancer, Depression, Diabetes, Arthritis, Sex, Age Category, Height, Weight, BMI, Smoking history, Alcohol Consumption, Fruit Consumption, Green Vegetables Consumption and Fried Potato Consumption. 

The pandas python library was used to import the dataset into the python environment used to train the models and the .info() method was used to check the information of the dataset which led to discovery that 12 columns out of the total 19 columns are of the object (string) datatype while the rest 7 columns are of the float64 (float with 64 bit precision) datatype. 


\subsection{Dataset Pre-processing}
The steps taken to pre-process the dataset to prepare it for model training are as follows:

\begin{itemize}
    \item Undersampling
    Data imbalance is a common issue faced when training models in the field of data science and machine learning. The model tends to assign more significance to the majority class and fails to properly represent the minority class. There are several known methods to address class imbalance some of which are random undersampling, random oversampling, over-under sampling.

    Due to the large size of the dataset, it was decided not to use the oversampling method since it would further increase the size of the dataset. Also, oversampling techniques are known to have issues such as duplicating or creating new data points very similar to some samples in the minority class \parencite{Ali2019}. The same issue could also exist in the oversampling section of the over-under sampling so it was decided not to use that either.

\end{itemize}
\subsection{Dataset Analysis}

% \begin{figure}[h]
%     \centering
%    \includegraphics[width=0.9\linewidth]{images/Checkup Frequency.png}
%    \caption{}
%    \label{fig:checkup}
% \end{figure}

The graph in Figure \ref{fig:checkup} clearly shows an upward trend in the number of heart disease cases recorded. The height of the bars suggest that the number of cases is steadily increasing over time.
The data shows a significant increase in heart disease cases when comparing the past year to the previous 2-year, 5-year, and 5-plus-year bars. The past year bar is greater than the 2-year bar, suggesting a higher number of instances in the last year compared to the preceding two years.
The past year's bar also exceeds the average height of the 5-year bars, indicating that the current year's count is greater than the 5-year average.
Furthermore, the height of the past year bar exceeds the average height of the 5+year bars, indicating that the number of instances in the most recent year is higher than the long-term 5+ year average.
The overall number of heart disease cases is increasing, and the growth rate looks to be picking up. This is demonstrated by the increasing vertical gaps between the most current year's bar and the previous multi-year average bars.
Further observation also shows that the number of people who came for check-ups with heart disease within the past year was greater than those without, an incident that has not occurred in the past 5 years or more.

These findings suggest an increasing heart disease burden, which could deteriorate at a rapid pace.

 % \begin{figure}[h]
 %     \centering
 %     \includegraphics[width=0.7\linewidth]{images/Excercise Count.png}
 %     \caption{Exercise to Heart Disease Ratio}
 %     \label{fig:ex-heart}
 % \end{figure}

There is a noticeable difference in the number of people who regularly exercise and those who don't, as seen in Figure \ref{fig:ex-heart}. The study and results may be skewed by participation bias resulting from this disparity in sample numbers. 
It's interesting to note that the data indicates a higher proportion of people without heart disease than those who exercise regularly. On the other hand, there are more people with heart disease than not in the group of people who do not exercise. This implies that there may be a negative correlation between heart disease incidence and exercise.
Although there are more heart disease instances in the exercise group overall, the percentage of people with heart disease is lower in the exercise group than in the non-exercising group. This suggests that engaging in regular physical activity may help lower the risk of heart disease. However, these results should be regarded cautiously due to the variation in sample size. It would be helpful to conduct more studies with more evenly distributed groups to validate these findings.

 \begin{figure}[h]
    \centering
     \includegraphics[width=0.7\linewidth]{images/Plot of Skin Cancer in relation to Heart Disease.png}
     \caption{Skin Cancer to Heart Disease Ratio}
     \label{fig:skin-heart}
 \end{figure}

According to Figure \ref{fig:skin-heart}, the number of people with skin cancer but no heart problems outnumbers those who have both. 

The variation in skin cancer rates between those with and without heart disease suggests that there is no strong direct link between these two health concerns. The greater incidence of skin cancer in people without heart disease shows that both illnesses may develop separately. 

This evidence suggests that having heart illness does not always raise or decrease one's risk of developing skin cancer, and vice versa. The risk factors and underlying causes of these two illnesses are likely separate and unrelated.

 \begin{figure}[htb]
     \centering
     \includegraphics[width=0.7\linewidth]{images/Plot of Other Cancer in relation to Heart_Disease.png}
     \caption{Other Cancer to Heart Disease Ratio}
     \label{fig:other-heart}
 \end{figure}


The data in Figure \ref{fig:other-heart} shows that those with different types of cancer had a higher risk of developing heart disease (excluding skin cancer). Specifically, among persons with various cancers, there are greater cases of heart disease than cases without. This shows a link between various cancers and cardiac problems.

Interestingly, even among those who do not have other cancers, there is a high prevalence of heart disease. While this figure does not exceed the number of people in this category who do not have heart disease, it is close, demonstrating that heart disease is common in the general community.

 % \begin{figure}[!htb]
 %     \centering
 %     \includegraphics[width=0.7\linewidth]{images/Plot of Depression in relation to Heart_Disease.png}
 %    \caption{Depression to Heart Disease Ratio}
 %     \label{fig:depr-heart}
 % \end{figure}


The chart in Figure \ref{fig:depr-heart} comparing depression and heart disease prevalence reveals a notable pattern. Among individuals with depression, there is a higher incidence of heart disease compared to those who are depressed but don't have heart disease. This suggests a potential positive correlation between depression and heart disease.

It's important to note that this relationship could be influenced by various factors or variables such as age, blood pressure, lifestyle choices, and other health conditions. 

Interestingly, when examining the group without depression, we observe a larger overall population. Within this group, the number of individuals with heart disease is relatively high, exceeding 17,500 cases. This is significantly more than the approximately 6,000 cases of heart disease among those with depression.

 

Figure \ref{fig:diabetes} depicts the association between diabetes and heart disease, which is consistent with recognized medical understanding and expectations. The figures show a strong positive correlation between these two health disorders, showing the interconnectedness of metabolic and cardiovascular illnesses.

\begin{figure}[htb]
     \centering
     \includegraphics[width=0.7\linewidth]{images/Plot of Diabetes in relation to Heart_Disease.png}
     \caption{Diabetes to Heart Disease Ratio}
     \label{fig:diabetes}
 \end{figure}

Upon study of the data, we see that there is a substantially greater number of persons diagnosed with diabetes who also have heart disease (about 8,000) than those who have diabetes but do not have heart disease. This study confirms the commonly held medical belief that diabetes is a substantial risk factor for cardiovascular problems, as highlighted in a variety of medical publications, including \parencite{national-institute-of-diabetes}

Interestingly, the data for pre-diabetes or borderline diabetes patients show a notable trend. This group has a greater incidence of heart disease than those without. This finding emphasizes the necessity of early intervention and preventive measures, especially in the pre-diabetic stage, to reduce the chance of developing heart disease.

A particularly noteworthy component of the data is the group of women who reported diabetes solely during pregnancy, sometimes known as gestational diabetes. Surprisingly, this population has no documented incidences of heart disease. While this may appear owing to a variety of circumstances


 % \begin{figure}[htb]
 %     \centering
 %     \includegraphics[width=0.7\linewidth]{images/Plot of Arthritis in relation to Heart_Disease.png}
 %     \caption{Arthritis to Heart Disease Ratio}
 %     \label{fig:arth-heartdisease}
 % \end{figure}

The relationship between arthritis and heart disease, as revealed by Figure \ref{fig:arth-heartdisease}, presents an intriguing and somewhat unexpected correlation. According to \parencite{Senthelal}, arthritis is defined as an acute or chronic joint inflammation, characterized by symptoms such as pain, stiffness, decreased range of motion, and joint deformities. Given this definition, one might not immediately associate arthritis with an increased risk of heart disease. However, the data presented in the chart challenges this initial assumption and provides valuable insights into the complex interplay between these two health conditions.

The chart reveals a surprising trend: individuals who have both arthritis and heart disease outnumber those who have arthritis without heart disease. This positive relationship between the two variables is unexpected and warrants further investigation. While arthritis and heart disease might seem unrelated at first glance, this data suggests a more intricate connection between these conditions than previously assumed.

Several factors could potentially explain this observed correlation. One significant consideration is the impact of arthritis on an individual's ability to engage in physical activity. The pain and reduced mobility associated with arthritis may make it extremely difficult for affected individuals to exercise regularly. This limitation could lead to a more sedentary lifestyle, potentially resulting in obesity - a well-known risk factor for cardiovascular disease. Furthermore, obesity can contribute to high blood pressure, another significant risk factor for heart disease.


 \begin{figure}[h]
     \centering
    \includegraphics[width=0.7\linewidth]{images/Relationship between Age and Heart Disease.png}
     \caption{Age to Heart Disease Ratio}
    \label{fig:age-heart}
 \end{figure}

Figure \ref{fig:age-heart} shows how age and heart disease are related to each other. A definite and meaningful pattern comes to the fore, showing a strong direct association between old age and the possibility of getting heart disease. This is particularly noticeable in people aged 50 years and above.

From the age group of 50-54, there has been an obvious increase in cases of heart disease. Such upward movement does not stop within this range but rather continues progressively until it peaks at the category for those aged 80 years and above. The incremental increase in heart diseases across these ages underscores the magnitude of ageing on cardiovascular health.

This observed pattern aligns with established medical knowledge about the effects of aging on the cardiovascular system. According to \parencite{rodgers} "Chances for the occurrence of Cardiovascular Disease (CVD) rise as men or women grow older hence it is associated with a general decline in sex hormones mainly testosterone and estrogen".

As individuals grow older, various physiological changes occur that can contribute to an increased risk of heart disease. These changes may include the stiffening of arterial walls, decreased efficiency of the heart muscle, and cumulative effects of lifestyle factors over time. 


 % \begin{figure}[h]
 %     \centering
 %    \includegraphics[width=0.8\linewidth]{images/Relationship between Smoking History and Heart Disease.png}
 %     \caption{Smoking to Heart Disease Ratio}
 %     \label{fig:smoke-heart}
 % \end{figure}

The bar chart in Figure \ref{fig:smoke-heart} portraying smoking habits and heart diseases incidences shows a compelling and significant correlation between the two. Some interesting pattern emerges in this regard that emphasizes the profoundness of the impact of smoking on cardiovascular health.

The data clearly shows that people who smoke have higher rates of heart disease than non-smokers with such cases. This strong observation gives enough proof for the well-known fact that smoking has adverse effects on cardiovascular system \parencite{fda_2021}. The figure provided quantifies and strengthens this relationship; thus, it is an effective way to understand practical implications of smoking patterns.

This link between smoking and heightened risk factor for heart attack therefore fits in line with volumes of medical research as well as public health warnings regarding tobacco hazards. Smoking negatively affects the cardiovascular system in many ways including causing harm to blood vessels' walls, raising blood pressure levels, reducing oxygen supply to the heart muscle leading to its perfusion failure, and encouraging formation of blood clots \parencite{jayet2005}


 % \begin{figure}[h]
 %     \centering
 %     \includegraphics[width=0.9\linewidth]{images/Relationship between Alcohol Consumption and Heart Disease.png}
 %     \caption{Alcohol to Heart Disease Ratio}
 %     \label{fig:alco-heart}
 % \end{figure}

Figure \ref{fig:alco-heart} shows that the correlation between alcohol and heart disease is complicated, sophisticated and challenging to make a simple conclusion. Data given gives a range of insights some of which were expected and others few surprises that make it difficult to draw clear conclusions about how alcohol affects health in general.

For non-drinkers or those with an alcohol reading of 0.0, the trend was quite predictable. This helps one to compare different levels of alcohol consumption as they are presented against a more understandable group.

A fascinating pattern emerges as we move along the spectrum of alcohol intake  0.1 to 20.0. There is a higher rate of heart disease among those who have readings within this range than those who do not have it. This corresponds to certain findings that drinking some alcoholic beverages, even moderately can be detrimental for heart health  \parencite{piano2017}. Nonetheless, it should be noted that these statistics fail to take into account other confounding factors like general diet, exercise frequency and genetic predispositions among other elements that affect these outcomes

Between 24.0 and 30.0, we can notice a considerable exparencitement in the data with respect to alcohol consumption rates. Surprisingly, there seems to be an even balance within this range between individuals who have heart disease and those who don't. This gender equality challenges common beliefs about the association between drinking habits and risk of developing heart diseases.
Astonishingly, an astonishing result comes from the population whose alcoholic percentage is scored at 30.0. However, this class has lower levels of cardiovascular disease than that without them; a phenomenon that appears counterintuitive. Such an unexpected finding raises several questions and possible research directions.


 \begin{figure}[h]
     \centering
     \includegraphics[width=0.7\linewidth]{images/Relationship between Height and Heart Disease.png}
     \caption{Height to Heart Disease Ratio}
     \label{fig:height-heart}
 \end{figure}

The graph in Figure \ref{fig:height-heart} presents a fascinating relationship between high and the frequency of heart disease. Within the height range of 160.0 cm to 170.0 cm, there is a striking pattern: there are more people with no heart disease than those that have it. This trend proposes that individuals within this height bracket might have less susceptibility to contracting heart diseases but additional confirmation is needed in regard to causality.
However, at the 168.0 cm mark, there is deviation from this pattern. But apart  From this point, we where seeing an upward-sloping line that connects height and incidence of cardiovascular diseases. This indicates that as height increases,so is the  likeliness to be suffering from heart diseases.

This pattern becomes even stronger in the height ranges of 173.0 cm to 183.0 cm and above which we find highest rate ratios of individuals with heart problems against those without them respectively. Because mainly tall persons over a height of 173 cm are very prone to cardiovascular disorders should they occur
It is important to note that the meaning of these observations could have profound significance on our understanding of how physical features relate to heart related issues. While this data shows a connection, it does not necessarily mean that one caused the other. These trends might have been accounted for by factors such as heredity, way of life and surroundings.


 % \begin{figure}[h]
 %     \centering
 %     \includegraphics[width=0.7\linewidth]{images/Relationship between Weight, BMI and Heart Disease.png}
 %     \caption{Weight and BMI by Heart Disease}
 %     \label{fig:weight-bmi}
 % \end{figure}

The scatter plot in Figure \ref{fig:weight-bmi} under discussion shows how complex weight, Body Mass Index (BMI), and heart disease are intertwined. This plot uses two colours (blue and yellow) to represent the presence or absence of a heart disease among the people and highlight how these variables are spread throughout the population being studied.
The central cluster consists of most data points within a weight range from 50 to 170 kg and BMI range from 20 to 60. This implies that irrespective of whether they had heart diseases or not, majority of study participants who form this middle group do fall into these categories. Therefore, it can be inferred that there is equal distribution of those having no heart disease in comparison with those having them in each group for both body weight as well as body mass index (BMI).

However, it is important to note that there are outliers observed at higher levels; weights between 200 and 290 units, as well as BMIs between 80-100. Typically, these extreme values represent individuals with morbid obesity. Therefore the fact that such outliers exist within our records implies that while extremely obese persons may be rare in this study population at large they still occur occasionally thereby causing major health implications.
The fact that heart disease cases are uniformly spread throughout the main cluster suggests that weight and BMI are not the only determinants of heart disease. Other factors could include genetic predisposition, lifestyle, stress levels and other health conditions like hypertension or diabetes.

Even though it is popular to use BMI as a measure of body composition, it has got some limitations. For instance, there is no distinction between muscle mass and fat mass which could lead to misclassification particularly for athletes or people with high muscle mass.
Of interest are outliers at the higher end of the weight and BMI ranges. The presence of these individuals shows that extreme obesity may have an enhanced likelihood of causing heart disease; however, based on what you've described this range appears to have few numbers. This supports calls for more focused interventions as well as closer monitoring of cardiovascular health in severely obese people.


 % \begin{figure}[h]
 %     \centering
 %     \includegraphics[width=0.8\linewidth]{images/Heart Disease Distribution by Age Category and Sex.png}
 %     \caption{Age and Sex by Heart Disease}
 %     \label{fig:age-sex}
 % \end{figure}

A striking visualization of heart disease spread across ages by sex is shown in Figure \ref{fig:age-sex}. The chart uses different colors, yellow for those with heart disease and blue for those without; this effectively demonstrates the prevalence of heart diseases in each category.

Another main finding from this chart was that there was a general increase with age in terms of having a heart disease which happened equally to both males and females. This corresponds to well-established scientific knowledge that advanced age is a major risk factor for cardiovascular illness as illustrated in Cardiovascular Risks Associated with Gender and Aging by \parencite{rodgers}. During aging, the circulatory system undergoes various physiological changes including arterial stiffening along with reduced efficiency of cardiac muscle thus enhancing the likelihood of developing coronary problems.

Nevertheless, there are some gender differences when it comes to heart diseases as depicted on the chart. According to the chart, males had more instances of heart disease than women did at every stage of their lives. This difference in rate between men and women regarding how many cases they have may have important policy implications and merits further study if public health strategies are to be formulated properly.

In total, many things may be responsible for this gender discrepancy. Thus, a number of issues resulted in such gender difference as the traditional association of male with higher cardiovascular disease risk due to elevated rates of smoking, alcohol intake and work stress. Besides, menopause could lead to hormonal variations between males and females; hence pre-menopausal women having reduced levels of heart attacks.

The fact that there is a higher prevalence rate of heart diseases in males shown by the chart does not mean that it is less serious among females. Actually, heart disease continues to be one of the major causes of death worldwide among both men and women \parencite{worldheart}. Given that symptoms can present differently in men than they do in women when dealing with heart ailments like hypertension or cardiomyopathy there are chances of lower incidence rates among the female folks leading to misdiagnosis or late treatment.


 \begin{figure}[h]
     \centering
     \includegraphics[width=0.8\linewidth]{images/Heart Disease Distribution by Weight and Diabetes.png}
     \caption{Weight and Diabetes by Heart Disease}
     \label{fig:weight-diabetes}
 \end{figure}

The scatter plots in Figure \ref{fig:weight-diabetes} provide an intricate picture of the distribution of heart disease across varying weight categories and diabetes. By depicting blue dots as people without heart diseases and yellow dots as those with heart diseases, these graphs effectively demonstrate how common is the condition among different groups.
For people who are not diabetic, the first plot indicates that there are slightly more individuals without the disease (blue) than those with it (yellow). This means that in the absence of diabetes, this risk may be somewhat lower.
The other graph, however, depicts a higher number of yellow dots, indicating a high incidence rate for heart diseases in diabetics. This is a clear indication that diabetes increases susceptibility to cardiovascular complications such as heart attack. The second plot compared to the first has many more yellow dots thus indicating greater impact on cardiac health from diabetes.

The third plot, focusing on individuals with pre-diabetes or borderline diabetes, shows a higher prevalence of yellow dots compared to blue dots. This suggests that even pre-diabetic conditions are associated with an increased risk of heart disease. This emphasizes the importance of early intervention and lifestyle modifications for individuals in this category to potentially prevent or delay the onset of both full diabetes and heart disease.
The fourth plot, representing females who experienced diabetes only during pregnancy (gestational diabetes), shows a higher prevalence of blue dots over yellow dots. The data points are clustered in a narrower weight range (50 to 100 kg) with outliers up to 170 kg. This suggests that gestational diabetes may have a different relationship with heart disease risk compared to other forms of diabetes. The higher prevalence of individuals without heart disease in this group could indicate that the transient nature of gestational diabetes might have less long-term impact on heart health compared to chronic diabetes.

The report has found out that diabetes is among the factors that increase risks of heart disease. Therefore, diabetes management is crucial for cardiovascular health.
Additionally, the information reveals that pre-diabetes or borderline diabetes is also linked to an increased risk of heart disease. This observation brings up early intervention and prevention strategies implying that blood sugar control should not wait until someone is fully diabetic but should start immediately even before there are any symptoms of becoming diabetic since it would have significant benefits on the health of the heart.
Consuming weight alone does not sufficiently indicate whether a person might develop heart diseases because various plots show similar patterns of clustering. However, higher weights having outliers suggest that excessive obesity may present additional medical conditions. Thus, understanding between weight and chances of having heart attacks necessitates a shift in thinking away from focusing only on weight numbers as chief indicators of good health into other holistic measures related to physical well being.

The analysis also reveals that gestational diabetes may have a different relationship with heart disease risk compared to other forms of diabetes. This distinction could be attributed to the temporary nature of gestational diabetes. This finding suggests that the impact of different types of diabetes on heart health may vary, and healthcare strategies should be tailored accordingly.

\chapter{Methodology}
\section{Models}
\subsection{Mathematical Description of Models}
\subsubsection{Logistic Regression}

The logistic regression model is expressed in the following formula:

\[P(y = 1 \mid \mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{w}^T \mathbf{x} - b)}\]

Where P(y = 1 | x) stands for the probability of output y being 1 given an input x. w stands for the coefficients of the input features and \(\mathbf{w}^T\) stands for the transpose of w. x stands for the vector of input features. b stands for the intercept (bias). exp stands for the exponential function

The logistic function follows the formula:

\[\sigma(z) = \frac{1}{1+\exp({-z})}\]

In the logistic regression model, the goal is to ensure that the data is linearly separable i.e the output is a probability between 0 and 1. To ensure this, the linear combination \(\mathbf{w}^Tx + b\) also referred to as the equation of a plane is passed through the logistic function. 

Logistic regression also has a decision rule which guides the classification output. If the predicted probability is greater than or equal to 0.5, the output is defined as 1, else, it is defined as 0. It can be represented mathematically as:

\[\hat{y} = \begin{cases} 1 & \text{if } P(y = 1 \mid x) \geq 0.5 \\ 0 & \text{if } P(y = 1 \mid x) < 0.5 \end{cases} \]

\subsubsection{Random Forest}

Mathematically,

For a p-dimensional random vector \(X = (X_1,...X_p)^T\) which represents the real valued input or predictor variables and a random variable Y representing real valued response, we assume an unknown joint distribution \(P_{XY} (X, Y)\).  Now, the aim is to find a prediction function \(f(x)\) for predicting Y which is determined by a loss function \(L(Y, f(X))\) and expected to minimize the expected value of the loss in the form:  

\[E_{XY} (L(Y, f(X)))\]

Where the subscript XY denotes the expectation with respect to the joint distribution of X and Y. Intuitively, L(Y, f(X)) is a measure of how close f(X) is to Y. The ideal choices for L are the squared error loss \(L(Y, f(X)) = (Y - f(X))^2\) for regression and zero-one loss for classification:



\[
L(Y, f(X)) = I(Y \neq f(X)) = 
\begin{cases} 
0 & \text{if } Y = f(X) \\
1 & \text{otherwise}
\end{cases}
\]



This turns out that minimizing \( E_{XY} (L(Y, f(X)))\) for squared error loss gives the conditional expectation.

\[f(X) = E (Y \mid X = x)\]

The above is the regression function, in a classification, if a set of possible values of Y is denoted by \(\zeta\), minimizing  \(E_{XY} (L(Y, f(X)))\) for zero-one loss gives:


\[f(X) = \arg \max_{y \in \zeta} P(Y = y|X = x)\]

which is known as the Bayes rule.

Ensembles construct f in terms of a collection of the so-called base learners \(h1(x),...hj(x)\) and these base learners are combined to give the ensemble predictor $f(x)$. In regression, these base learners are averaged as follows:

\[f(X) = \frac{1}{J} \sum_{j=1}^{J} h_j (x),\]

While in classification, \(f(x)\) is the most frequently predicted voting class given as:

\[f(X) = \underset{y \in \zeta}{\operatorname{argmax}} \sum_{j=1}^{J} I(y = h_j(x))\]

The mathematical derivation above deals with loss functions and the construction of ensemble models, such as random forests, for both regression and classification tasks. The first equation shows the expected loss function \( E_{XY} (L(Y, f(X)))\), where \(L(Y, f(X))\) measures how close the predicted value \(f(X)\) is to the true value \(Y\), and the expectation is taken over the joint distribution of X and Y. For regression, the squared error loss \(L(Y, f(X)) = (Y - f(X))^2\) is minimized, leading to the optimal solution \(f(X) = E (Y \mid X = x)\), which is the conditional expectation. For classification, the zero-one loss is used, where \(L(Y, f(X)) = I(Y \neq f(X))\), meaning the loss is 0 when the prediction is correct and 1 otherwise. Minimizing this loss gives the Bayes rule, \(f(X) = \arg \max_{y \in \zeta} P(Y = y|X = x)\), which is most likely class given the input.

In random forests, the ensemble prediction \(f(X)\) is constructed by averaging the predictions of base learners \(h_j(x)\) in regression, given by \(f(X) = \frac{1}{J} \sum_{j=1}^{J} h_j (x)\). For classification, the predicted class is determined by majority voting, where, \[f(X) = \underset{y \in \zeta}{\operatorname{argmax}} \sum_{j=1}^{J} I(y = h_j(x))\] being an indicator function counts how many base learners predicted each class. This derivation shows how ensenble models combine individual base models to improve overall prediction accuracy


\subsubsection{XGBoost}

With speed and efficiency in mind, the gradient boosted decision tree implementation known as XGBoost was developed \parencite{raschka-2020}. The XGBoost is a decision tree ensemble based on the gradient boosting and is expected to be scalable. 


\[L_{xgb} = \sum_{i=1}^{N} L(y_1, F(X_i)) + \sum_{m=1}^{M} \Omega h_m \]


\[\Omega(h) = \gamma T + \frac{1}{2} \lambda\mid \mid\textbf{w}\mid \mid ^2\]

Mathematically, the XGBoost is based on the gradient boosting as follows:


XGBoost is based on gradient boosting as follows:
Gradient boosting builds an additive approximation of F*(X) as a weighted sum of functions

\[F_m(X) = F_{m-1 (X)} + \rho_mh_m(X)\]

Where \(\rho_m\) is the weight of the math function, \(h_m(X)\)

Next, the model is constructed iteratively but first a constant is obtained as follows:

\[F_0(X) = \arg \min_{\alpha} \sum_{i=1}^{N} L(y_i, \alpha)\]

Therefore the subsequent models are expected to minimize

\[
(\rho_m h_m(X)) = \arg\min_{\rho, h} \sum_{i=1}^{N} L(y_i, F_{m-1}(X_i) + \rho h(X_i))
\]


But instead of solving the optimization problem head on, each \(h_m\) can be seen as a greedy step in a gradient descent optimization for F*. Hence, for each model, \(h_m\), is trained on a new dataset \(D = \{X_i, r_{mi}\}_{i=1}^{N}\), where the residuals \(r_{mi}\) is gotten by:

\[r_{mi} = [\frac{\delta L (y_i, F(X))}{\delta F(X)}]_{F(X)=F_{m-1}(X)}\]

In the equations above, we are looking at the gradient boosting process used in XGBoost, which builds an additive model to approximate the true function F*(X). The main idea is to iteratively improve the model by adding new functions that minimize the residual errors of the previous models. The equation \(F_m(X) = F_{m-1 (X)} + \rho_mh_m(X)\) shows that at each step \(m\), a new base learner \(h_m(X)\) is added to the previous model \(F_{m - 1}(X)\), with \(\rho_m\) representing the weight assigned to this learner. The base learner \(h_m(X)\) is designed to correct the errors made by \(F_{m - 1}(X)\).

In the next step, \(F_0(X) = \arg \min_{\alpha} \sum_{i=1}^{N} L(y_i, \alpha)\), calculates the initial constant value \(F_0(X)\), which minimizes the overall loss \(L(y_i, \alpha\) for the dataset, where \(L\) is the loss functino and \(y_i\) are the true labels.

The aim is to train subsequent models to minimize the residuals of the previous models. Following the model sequence, \((\rho_m, h_m(X)) = \arg\min_{\rho, h} \sum_{i=1}^{N} L(y_i, F_{m-1}(X_i) + \rho h(X_i)\), shows the goal of each iteration whcih is to find the optimal base learner \(h_m(X)\) and its weight \(\rho_m\) that minmize the loss between the true values \(y_i\) and the current predictions \(F_{m-1}(X)\). Instead of solving this optimization directly, XGBoost takes a greedy step by training each base learner \(h_m(X)\) on the residuals, \(r_{mi}\), calculated as the gradient of the loss function with respect to the current model's predictions. The last equation, \(r_{mi} = [\frac{\delta L (y_i, F(X))}{\delta F(X)}]_{F(X)=F_{m-1}(X)}\) shows how the residuals are derived, essentially representing how much each model is off in its predictions, which the next learner tries to correct. Generally, \(F(X)\) is the model's prediction at step \(m\), \(m\)  is the weight for the new base learner \(h_m(X)\), \(L\) is the loss function, and \(r_{mi}\) are the residuals or the negative gradients of the loss function which guide the model to better predictions with each iteration.



\subsubsection{Decision Tree}

SPLITTING CRITERIA FOR THE DECISION TREES

\begin{itemize}
    \item IMPURITY BASED CRITERIA: Provided we are presented with a random variable \(x\), with \(k\) discrete values, spread according to \(P = (p_1, p_2,....,p_k)\), an impurity measure is a function

    \(\Phi: [0, 1]^k \hookrightarrow R  \text{ which satisfies the conditions:}\)

    \begin{itemize}
        \item \(\Phi (P) \geq 0 \)
        \item \(\Phi(P) \text{is minimum if } \exists \text{ i such that component} P_i = 1 \)
        \item \(\Phi (P) \text{ is maximum if } \nu \text{ i}, 1 \leq i \leq k, P_1 = \frac{1}{j}\)
        \item \(\Phi (P) \text{ is symmetric with respect to components of P. }\)
        \item \(\Phi \text{ is smooth in its range.} \)
    \end{itemize}

Given a training set S, the probability vector of the target attribute y is defined as:

\[P_y(S) = (\frac{\mid \delta_{y=c_1}S \mid}{\mid S \mid},....,\frac{\mid \delta_{y=c_{dom(y)}} S\mid}{\mid S \mid}\]

The goodness-of-split due to discrete attribute \(a_i\) is defined as reduction in impurity of the target attribute after partitioning S according to the values \(v_{ij} \epsilon dom (a_i)\):

\[
\Delta\Phi(a_i, S) = \Phi(P_y(S)) - \sum_{j=1}^{\mid \text{dom}(a_i) \mid} \frac{\mid \sigma_{a_i = v_{i,j}} \mid}{\mid S \mid} \cdot \Phi(P_y(\sigma_{a_i = v_{i,j}} S))
\]

\item INFORMATION GAIN
This is an impurity-based criterion that uses the entropy measure as impurity measure

\[
\text{InformationGain}(a_i, S) = \text{Entropy}(y, S) - \sum_{v_{i,j} \in \text{dom}(a_i)} \frac{|\sigma_{a_i = v_{i,j}}|}{|S|} \cdot \text{Entropy}\left(y, \sigma_{a_i = v_{i,j}} S\right)
\]

Where:

    \[
\text{Entropy}(y, S) = \sum_{c_j \in \text{dom}(y)} - \frac{|\sigma_{y = cj}S|}{|S|} \log_2 \left(\frac{|\sigma_{y = c_j}|}{|S|}\right)
\]

\item GINI INDEX: This measures the divergence between the probability distribution of the target attributes values


\[
\text{Gini}(y, S) = 1 - \sum_{c_j \in \text{dom}(y)} \left(\frac{|\sigma_{y = c_j}|}{|S|}\right)^2
\]

Therefore, the criterion of evaluation for selecting the attribute \(a_i\) is given as:


\[
\text{GiniGain}(a_i, S) = \text{Gini}(y, S) - \sum_{v_{i,j} \in \text{dom}(a_i)} \frac{|\sigma_{a_i = v_{i,j}}S|}{|S|} \cdot \text{Gini}(y, \sigma_{a_i = v_{i,j}}, S)
\]

When training decision trees, the goal is to find the best attribute to split a dataset, and this process is guided by impurity-based criteria such as Information Gain and Gini Gain. The probability vector \(P_y (S)\) represents the proportion of different classes y within the training set S. The equation for the goodness-of-split for a discrete attribute \(a_i\) quantifies the reduction in impurity after partitioning the dataset based on the values of attribute \(a_i\), where \(\text{dom}a_i\) is the domain of possible values that attribute \(a_i\) can take. 

The change in impurity \(\delta\phi(a_i, S)\) is calculated as the impurity before the split \(\phi(P_y(S))\) minus the weighted sum fot he impuriries of the subsets created by the split. the Subsets are formed based on each value \(v_j\epsilon\text{dom}(a_i)\), and \(\phi(P_y(\rho_{a_i=v_{i,j}}S\) represents the impurity in each partition. 
For Information Gain, the impurity is measured using entropy. The entropy of the target variable y in the dataset S is defined as \(\text{Entropy}(y, S) = \sum_{c_j \in \text{dom}(y)} - \frac{|\sigma_{y = cj}S|}{|S|} \log_2 \left(\frac{|\sigma_{y = c_j}|}{|S|}\right)
\) where \(P(y=j)\) is the probability of class \(j\) in the dataset. Information Gain measures the decrease in entropy afer splitting on \(a_i\).

The Gini Index is another impurity measure, defined as

\(\text{Gini}(y, S) = 1 - \sum_{c_j \in \text{dom}(y)} \left(\frac{|\sigma_{y = c_j}|}{|S|}\right)^2\) which captures the likelihood of misclassification. Similar to Information Gain, Gini Gain evaluates the reduction in the Gini index after splitting the dataset on attribute \(a_i\). Both metrics guide the decision tree in selecting the best attribute for a split by measuring how much the impurity is reduced after partitioning the data.

\end{itemize}

\subsubsection{AdaBoost}

If we have a group of weak classifiers \(\left\{ \phi_\mu(x): \mu = 1, \ldots, M \right\}\), with labeled data \(X = \left\{ (x_i, y_i): i = 1, \ldots, N \right\}\) with \(y_i \in \left\{ \pm 1 \right\}\), we have an output classifier:

\[S(x) = \text{sign}\left( \sum_{\mu=1}^M \lambda_\mu \phi_\mu(x) \right)\]

Where \(\left\{ \lambda_\mu \right\}\) are learned weights.



\section{Model Training}
\subsection{Preprocessing Steps}

\subsubsection{Label Encoding}
Label encoding is one of the most important preprocessing steps in a machine learning workflow. Machine learning algorithms are able to handle numerical input better than categorical input hence the label encoder is used to convert categorical types of data into it's equivalent numerical form. It does this by assigning a unique integer to each category within a feature.

In this study, the LabelEncoder method from scikit-learn was used to convert all object columns (categorical type columns) into their numerical form.

\subsubsection{Feature Selection}
Feature selection is a process in machine learning where features that do not contribute meaningfully to the output (noisy data) are removed, leaving behind a subset of relevant features which will be used in model construction. This process reduces the complexity of the dataset and can help the model generalize better to unseen data.

In this study, instead of using the various methods available for feature selection such as statistical methods (chi-square, anova), wrapper methods(forward selection, backward elimination, recursive feature elimination) or embedded methods (lasso regression, decision trees), a different approach was taken.

Each feature in the dataset was individually researched and selected based on the real world importance to the target variable.

\subsubsection{Splitting}

In machine learning, two types of datasets are needed for successful model training - training data and testing data. Once other pre-processing steps are concluded on the dataset, the next step is to split the data into a training and testing set. 

The model will be trained on the training data and tested on the testing (unseen) data to measure it's performance

\subsection{Metrics}

The evaluation of the machine learning models in this study involved the usage of various performance metrics to assess their effectiveness. The following metrics were used:

\subsubsection{Accuracy}
Accuracy measures the proportion of correctly predicted instances among all the instances in the dataset. It provides an overall assessment of the model's predictive accuracy, regardless of the class distribution. The accuracy metric is computed as the ratio of correctly classified instances to the total number of instances.

\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]

Where:

\begin{itemize}
    \item \(TP\) is the number of true positives,
    \item \(TN\) is the number of true negatives,
    \item \(FP\) is the number of false positives,
    \item \(FN\) is the number of false negatives
\end{itemize}

\subsubsection{Balanced Accuracy}
Balanced Accuracy is particularly useful when dealing with imbalanced datasets. It is the average of the true positive rate (recall) and the true negative rate (specificity), giving equal importance to both classes. This metric helps to account for class imbalance by considering both the sensitivity of the positive class and the specificity of the negative class.

\[
\text{Balanced Accuracy} = \frac{1}{2} \left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP} \right)
\]

\subsubsection{Precision}
Precision quantifies the proportion of correctly predicted positive instances out of the total instances predicted as positive. It is a measure of the model's ability to avoid false positives, i.e., correctly identifying instances as positive when they truly are. Precision is calculated as the ratio of true positives to the sum of true positives and false positives.

\[
\text{Precision} = \frac{TP}{TP + FP}
\]

\subsubsection{Recall} 
Recall, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out of the total actual positive instances in the dataset. It represents the model's ability to identify all positive instances without missing any. Recall is calculated as the ratio of true positives to the sum of true positives and false negatives.

\[
\text{Recall} = \frac{TP}{TP + FN}
\]

\subsubsection{F1} 
The F1 score is a harmonic mean of precision and recall. It provides a balanced measure of the model's performance by considering both precision and recall. The F1 score combines precision and recall into a single value, providing an overall evaluation of the model's ability to balance between true positives and avoiding false positives and false negatives.

\[
\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]

\subsubsection{ROC AUC}
ROC AUC (Receiver Operating Characteristic Area Under the Curve) is a performance metric commonly used for binary classification problems. It evaluates the model's ability to discriminate between positive and negative instances across various classification thresholds. The ROC curve is created by plotting the true positive rate (recall) against the false positive rate. The ROC AUC score represents the area under the ROC curve and provides an aggregate measure of the model's discriminative power.

\[
\text{ROC AUC} = \int_{0}^{1} \text{TPR}(f) \, d\text{FPR}(f)
\]


\chapter{Results}

The study built 5 models for cardiovascular risk prediction and the results are evaluated with several metrics such as Accuracy, Balanced Accuracy, Recall, F1 Score, Precision, and ROC-AUC.


\section{Accuracy}
The accuracy metric is used to measure the proportion of correctly predicted instances among all the instances in the dataset. It is observed from Table \ref{table:all_model_performance} that model with the highest accuracy was AdaBoost (73.2706), followed closely by Logistic Regression (73.2105) and XGBoost (72.6099). This indicates that the top two performing models using the accuracy metric are more capable in classifying cardiovascular risk cases compared to the least two performing models - Random Forest (66.2929) and Decision Tree (64.8914).

\section{Balanced Accuracy}

This variant of accuracy gives a more balanced evaluation by accounting for both sensitivity and specificity. Here, we also observe a similar trend to the Accuracy metric with AdaBoost (73.3198) achieving the highest balanced accuracy followed by Logistic Regression (73.2547). This supports our previous observation that the two models are are capable of classifying cardiovascular risk cases better than the other models in the study.


\section{Recall}

The recall metrics shows the model's ability to correctly identify positive instances which is very crucial in the healthcare industry where there are serious consequences for misclassifying a high risk patient. The AdaBoost model demonstrated the best performance with the highest recall (73.4695) closely followed by other models: Logistic Regression (73.3758) and XGBoost (72.9248).

With this evaluation, it is seen that the AdaBoost model is a preferred choice when dealing with the healthcare industry which is the case in this study.

\subsection{F1 Score}

The F1 score indicates a balance between the Precision and the Recall metrics. The AdaBoost model was the top performing model (73.2706) while LR performed similarly(73.2105).

\subsection{Precision}
Precision measures the accuracy of the positive predictions made by the model.
AdaBoost (73.2362) and Logistic Regression (73.1835) were the best performing models in this metric, indicating that they have a high rate of correctly identified positive cases. 

\subsection{ROC-AUC}
The ROC-AUC score shows the model's ability to distinguish between classes. The ROC-AUC scores mirror the Balanced Accuracy results, with AdaBoost (73.3198) and Logistic Regression (73.2547) achieving the highest values. These scores confirm that these models have a strong ability to differentiate between high-risk and low-risk individuals.

\begin{table}[htbp]
    \centering
    \begin{tabular}{||c|c|c|c|c|c||} 
        \hline
        \multirow{Metrics} & \multirow{LR} & \multirow{RF} & \multirow{DT} & \multirow{AdaBoost} & \multirow{XGBoost} \\
        \hline\hline
        Accuracy & 73.2105 & 66.2929 & 64.8914 & 73.2706 & 72.6099\\
        \hline
        Balanced Accuracy & 73.2547 & 66.2920 & 64.8487 & 73.3198 & 72.6745 \\
        \hline
        Recall & 73.3758 & 66.2967 & 64.9181	& 73.4695 & 72.9248 \\
        \hline
        F1 score & 73.2105 & 66.2929 & 64.8914 & 73.2706 & 72.6099\\
        \hline
        Precision & 73.1835	& 66.2940 & 64.8472 & 73.2362 & 72.5456 \\
        \hline
        ROC-AUC & 73.2547 & 66.2920 & 64.8487 & 73.3198 & 72.6745 \\
        \hline
    \end{tabular}
    \caption{Performance metrics}
    \label{table:all_model_performance}
\end{table}

The results detailing the performance of each models based on each metric are summarized in \ref{table:all_model_performance}


\chapter{Conclusion}\label{ch:concl}

The dataset that has been used in this study consists of 19 columns and 308854 rows of data, all of which covers a wide range of cardiovascular risk factors such as smoking, alcoholic consumption, diabetes, heart disease and BMI.

After the thorough evaluation of all metrics, it is noted that the AdaBoost model consistently demonstrates good performance as well as the Logistic Regression model and XGBoost. The strong performance of the boosting methods can be linked to their ability to handle high-dimensional data and the effective exploitation of subtle patterns. The findings in the study strongly suggests that the use of ensemble methods i.e Adaboost, XGBoost are very effective in predicting cardiovascular risk. 

The simplest model, Logistic Regression, has also performed well indicating it's overall superiority for this type of classification task. The poorer performance of Random Forest and Decision Tree further demonstrate the difficulties these models have, especially when it comes to capturing complex and non-linear relationships in the data. This results suggests that simpler decision trees may struggle to differentiate between high- and low-risk patients without more sophisticated techniques like boosting, which can refine decision boundaries.

This study has clearly been able to show the effectiveness of the several models trained, but there still exists numerous directions for future research such as exploring additional cardiovascular-related features such as biomarkers or imaging data. Since ensemble methods like AdaBoost and XGBoost performed well, it is recommended that there be an addition of explainable AI techniques such as the Shapley Additive Explanations (SHAP) to provide more transparency in the decision making process of the models. This study could also explore the use of neural networks which may further improve the predictive accuracy as it can better capture complex patterns in larger datasets.


\printbibliography
\chapter{Appendix}
\begin{lstlisting}[language=Python]
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier

df = pd.read_csv('./data/CVD_cleaned.csv')

print(df)

print(df.info())

df.Heart_Disease.value_counts()

pos = df[df['Heart_Disease']=='Yes']
neg = df[df['Heart_Disease']=='No']

new_neg = neg.sample(n=24971,AC random_state=43)

dfa = pd.concat([pos, new_neg]).sample(frac=1, random_state=4).reset_index(drop=True)

def data_plots(data: pd.DataFrame, 
               x: str,
               y: str= None,
               title: str = None,
               xlabel: str = None,
               ylabel: str = None,
               hue: str = None,
               order: list = None) -> None:
    
    # define size
    plt.figure(figsize=(10, 5))
    
    # define countplot method
    sns.countplot(x=x, y=y, data=data, hue=hue, order=order)
    
    # add titles and labels
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)

    plt.xticks(rotation=45, ha='right')
    
    plt.savefig(f'{title}.png', format='png', dpi=300, bbox_inches='tight')

def cat_plots(data: pd.DataFrame, 
              x: str, 
              col: str, 
              hue: str,
              title: str,
              xlabel: str,
              ylabel: str,
              col_wrap: int = 2, 
              order: list = None,
              kind: str = 'count') -> None:
    
    # Define catplot method
    fctgrid = sns.catplot(data=data, x=x, hue=hue, col=col, kind=kind, height=6, aspect=1.2, col_wrap=col_wrap, order=order)
    
    # Add titles and labels
    fctgrid.fig.subplots_adjust(top=0.9)
    fctgrid.fig.suptitle(title, fontsize=16)
    fctgrid.set_axis_labels(xlabel, ylabel)
    
    fctgrid.savefig(f'{title}.png', format='png', dpi=300, bbox_inches='tight')
    
    # Show plot
    plt.show()

data_plots(dfa, x='Checkup', title='Checkup Frequency', xlabel='Checkup', ylabel='Count', hue='Heart_Disease')

data_plots(dfa, x='Exercise', title='Excercise Count', xlabel='Exercise', ylabel='Count', hue='Heart_Disease')

data_plots(dfa, x='Skin_Cancer', title='Plot of Skin Cancer in relation to Heart Disease', xlabel='Skin Cancer', ylabel='Count', hue='Heart_Disease')

data_plots(dfa, x='Other_Cancer', title='Plot of Other Cancer in relation to Heart_Disease', xlabel='Other Cancer', ylabel='Count', hue='Heart_Disease')

data_plots(dfa, x='Depression', title='Plot of Depression in relation to Heart_Disease', xlabel='Depression', ylabel='Count', hue='Heart_Disease')

data_plots(dfa, x='Diabetes', title='Plot of Diabetes in relation to Heart_Disease', xlabel='Diabetes', ylabel='Count', hue='Heart_Disease') 

data_plots(dfa, x='Arthritis', title='Plot of Arthritis in relation to Heart_Disease', xlabel='Arthritis', ylabel='Count', hue='Heart_Disease')

data_plots(dfa, x='Age_Category', title='Relationship between Age and Heart Disease', xlabel='Age_Category', ylabel='Count', hue='Heart_Disease')

data_plots(dfa, x='Smoking_History', title='Relationship between Smoking History and Heart Disease', xlabel='Smoking History', ylabel='Count', hue='Heart_Disease')

data_plots(dfa, x='Alcohol_Consumption', title='Relationship between Alcohol Consumption and Heart Disease', xlabel='Alcohol_Consumption', ylabel='Count', hue='Heart_Disease')

data_plots(dfa, x='Fruit_Consumption', title='Relationship between Fruit Consumption and Heart Disease', xlabel='Fruit_Consumption', ylabel='Count', hue='Heart_Disease')

height_order = [160.0, 163.0, 165.0, 168.0, 170.0, 173.0, 175.0, 178.0, 180.0, 183.0, 226.0]
data_plots(dfa, x='Height_(cm)', title='Relationship between Height and Heart Disease', xlabel='Height', ylabel='Count', hue='Heart_Disease', order=height_order)


plt.figure(figsize=(10, 5))
sns.scatterplot(data=dfa, x='Weight_(kg)', y='BMI', hue='Heart_Disease')

# Add titles and labels
plt.title('Relationship between Weight and Heart Disease')
plt.xlabel('Weight (kg)')
plt.ylabel('BMI')
plt.legend(title='Heart Disease')

# Show plot
plt.show()

# define age order
age_order = ['18-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80+']

cat_plots(data=dfa, 
          x='Age_Category', 
          col='Sex', 
          hue='Heart_Disease', 
          title='Heart Disease Distribution by Age Category and Sex', 
          xlabel='Age Category',
          ylabel='Count',
          order=age_order)

dfa.info()

cat_plots(data=dfa, 
          x='Smoking_History', 
          col='Skin_Cancer', 
          hue='Heart_Disease', 
          title='Heart Disease Distribution by Smoking and Skin Cancer', 
          xlabel='Smoking History',
          ylabel='Count')

cat_plots(data=dfa, 
x='Weight_(kg)', 
col='Diabetes', 
hue='Heart_Disease', 
title='Heart Disease Distribution by Weight and Diabetes', 
xlabel='Weight (kg)',
ylabel='Count',
col_wrap=2,
kind='strip')

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
cat_col = df.select_dtypes(include='object').columns
print(cat_col)

for i in df.columns:
    df[i] = label_encoder.fit_transform(df[i])
print(df)

for i in dfa.columns:
    dfa[i] = label_encoder.fit_transform(dfa[i])
print(dfa)

print(dfa.columns)

col_select = ['Age_Category', 'Smoking_History', 'Diabetes', 'Heart_Disease', 'Exercise', 'Sex', 'BMI']
dfa = dfa[col_select]

X = dfa.drop('Heart_Disease', axis=1)
y = dfa.Heart_Disease

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, roc_auc_score

Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=728)

import time
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

def models(X_train: pd.DataFrame,
           X_test: pd.DataFrame,
           y_train:pd.Series,
           y_test: pd.Series):
    
    # Define a list of models to train, including their names and classes
    k = 65
    lr = LogisticRegression(max_iter=1000, random_state=k)
    rf = RandomForestClassifier(random_state=k, n_estimators=200, n_jobs=-1)
    dtc = DecisionTreeClassifier(random_state=k, max_depth=500)
    xgb = XGBClassifier(random_state=k, tree_method="hist")
    et = ExtraTreeClassifier(random_state=k)
    ets = ExtraTreesClassifier(random_state=k)
    aboost = AdaBoostClassifier(random_state=k)
    lgb = LGBMClassifier(random_state=k)
    
    
    model_list = [('Logistic Regression', lr),
                  ('Random Forest', rf),
                  ('Decision Tree', dtc),
                  ('AdaBoost', aboost),
                  ('XGBoost', xgb)
                  ]
    
    scores_df = pd.DataFrame()  # Create an empty DataFrame to store evaluation scores
    
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']
    
    print('Starting loop')
    import logging
    for model_name, model in model_list:
        logging.info(model_name)
        print(f'Training {model_name}', end=' - Time taken to train (s): ')
        
        start_time = time.time()  # Record start time before training the model

        model.fit(X_train, y_train)  # Train the model

        end_time = time.time()  # Record end time after training the model
        total_time = end_time - start_time
        
        print(total_time)  # Print the time taken to train the model

        model_pred = model.predict(X_test)  # Make predictions on the test data

        # Evaluate the model's performance and store the scores in the DataFrame
        scores_df[model_name] =  [      accuracy_score(y_test, model_pred),
          balanced_accuracy_score(y_test, model_pred),
          precision_score(y_test, model_pred, average='weighted'),
          recall_score(y_test, model_pred, average='weighted'),
          f1_score(y_test, model_pred, average='weighted'),
          roc_auc_score(y_test, model_pred, average='weighted')]


    return scores_df
        
scores = models(Xtr,
                Xte,
                ytr,
                yte)

\end{lstlisting}


\end{document}